{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 7 - Passage au big data (2ème partie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dans le cadre de ce Notebook, nous allons parler de l'environnement Apache Spark. Ce notebook n'est donc pas applicable dans votre environnement \"classique\".**\n",
    "\n",
    "**Pour que le code foctionnne, il vous faut un environnement Spark correctement installé.**\n",
    "\n",
    "**N'essayez pas de faire fonctionner les cellules si votre environnemnt n'est pas correctement paramétré. Les cellules de code ont été passées au format RawNBConvert afin de ne pas rendre le Notebook inutilisable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark est un projet de la fondation Apache (actuellement dans sa version 3).\n",
    "\n",
    "Il a pour objectif de pallier les lacunes de Hadoop quant au traitement nécessitant de nombreux allers-retours.\n",
    "\n",
    "Si, malgré tous vos efforts, vous n’avez pas réussi à extraire des données de\n",
    "manière qu’elles tiennent dans votre mémoire RAM, le recours à une autre solution deviendra indispensable. Cette solution est Apache Spark.\n",
    "\n",
    "Cet environnement, développé à Berkeley, est un système de traitement distribué\n",
    "sur les noeuds d’une infrastructure big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous voulez tester Spark, je vous conseille d'essayer la version gratuite de Databricks qui est simple d'accèes :\n",
    "\n",
    "https://databricks.com/signup#signup/community\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 Le DataFrame de Spark SQL\n",
    "\n",
    "Nous allons nous concentrer sur Spark SQL. Ceci nous permettra d’introduire un objet : le DataFrame de Spark. Il s’agit d’un objet proche du RDD, mais qui permet de stocker de manière distribuée des données structurées, là où les RDD nous permettent de stocker des données non structurées.\n",
    "\n",
    "Il se rapproche très fortement du DataFrame de Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lancer votre session Spark\n",
    "Commençons par lancer une session Spark en utilisant dans un premier temps le\n",
    "package findspark et la classe SparkSession de pyspark.sql :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/07 22:02:02 WARN Utils: Your hostname, r2-60-gra7 resolves to a loopback address: 127.0.1.1; using 51.91.138.22 instead (on interface ens3)\n",
      "24/01/07 22:02:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/07 22:02:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/07 22:02:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# on importe findspark\n",
    "import findspark\n",
    "# on initialise findspark pour identifier nos chemins Spark\n",
    "findspark.init()\n",
    "# on importe SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "# on crée une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "         .appName(\"Exemples avec Python et Spark SQL\") \\\n",
    "         .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture des données (json, parquet, csv, hive)\n",
    "\n",
    "Spark vous permet de lire de nombreux types de données, que ce soit des données csv ou SQL classiques ou des données issues d’environnements big data. En voici quelques exemples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/07 22:02:10 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# lecture d’un fichier json\n",
    "df = spark.read.json(\"../data/data.json\")\n",
    "# lecture d’un fichier parquet\n",
    "df3 = spark.read.load(\"../data/data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark permet aussi d’utiliser des données issues de fichiers csv :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_idf=spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "              .option(\"delimiter\",\";\")\\\n",
    "              .option(\"inferSchema\", \"true\")\\\n",
    "              .load(\"../data/base-comparateur-de-territoires.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre format important dans le cadre du big data est le format Hive. Pour se\n",
    "connecter à une base Hive et soumettre du code SQL, on utilisera :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# on crée une session avec accès à Hive\n",
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "\n",
    "# on peut afficher une base\n",
    "spark.sql('show databases').show()\n",
    "\n",
    "# on peut créer une base\n",
    "spark.sql('create database base1')\n",
    "\n",
    "# on peut faire des requêtes en SQL\n",
    "spark.sql(\"select * from table1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi transformer un DataFrame Pandas en DataFrame Spark en utilisant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame([10,20,30])\n",
    "spark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipuler des DataFrames\n",
    "Il est très simple de manipuler des DataFrames de différentes manières. Spark part du principe que les calculs ne sont pas effectués chaque fois que vous soumettez du code. \n",
    "\n",
    "Ils le sont lorsque vous demandez explicitement à Spark de faire les calculs ou\n",
    "d’afficher les résultats. Ces opérations de calcul ou d’affichage sont appliquées avec les méthodes .collect() ou .show().\n",
    "\n",
    "Nous allons manipuler les données sur les communes d’Île-de-France. Nous\n",
    "voulons extraire des informations de ces données sur les communes de la région\n",
    "Île-de-France.\n",
    "\n",
    "Les codes ci-dessous nous\n",
    "permettent d’effectuer la plupart des manipulations dont nous avons besoin :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CODGEO', 'LIBGEO', 'REG', 'DEP', 'P14_POP', 'P09_POP', 'SUPERF', 'NAIS0914']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on récupère les données d’Ile-de-France\n",
    "# on a des titres dans la première ligne\n",
    "# le séparateur est le;\n",
    "# on demande à Spark d’inférer les types\n",
    "data_idf = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "           .option(\"delimiter\",\";\")\\\n",
    "           .option(\"inferSchema\", \"true\")\\\n",
    "           .load(\"../data/base-comparateur-de-territoires.csv\")\n",
    "# on peut afficher les 8 premiers noms de colonnes :\n",
    "data_idf.columns[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              LIBGEO|\n",
      "+--------------------+\n",
      "|       Saint-Gratien|\n",
      "|          Pierrelaye|\n",
      "|Saint-Cyr-en-Arthies|\n",
      "|      La Roche-Guyon|\n",
      "|       Villiers-Adam|\n",
      "|       Vallangoujard|\n",
      "|   Le Plessis-Gassot|\n",
      "|               Seugy|\n",
      "|  Villers-en-Arthies|\n",
      "|         Vaudherland|\n",
      "|   Asnières-sur-Oise|\n",
      "|       Saint-Maurice|\n",
      "|          Arnouville|\n",
      "|          Bray-et-Lû|\n",
      "|             Santeny|\n",
      "|  Le Plessis-Trévise|\n",
      "|              Bezons|\n",
      "|      Butry-sur-Oise|\n",
      "|           Beauchamp|\n",
      "|            Banthelu|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# on sélectionne une colonne et on affiche le résultat\n",
    "data_idf.select(\"LIBGEO\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les opérations ci-dessus sont stockées en mémoire et ne renvoient rien. C’est\n",
    "uniquement lorsqu’on ajoute show() ou collect() que les opérations sont\n",
    "effectuées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on crée un DataFrame par opération avec deux colonnes dont une colonne\n",
    "# qui indique si la commune est dans Paris\n",
    "data_reduced = data_idf.select(\"P14_POP\", data_idf[\"LIBGEO\"].startswith(\"Paris\"), \"LIBGEO\")\n",
    "# on peut aussi travailler sur les colonnes\n",
    "# on peut renommer une colonne :\n",
    "data_col = data_idf.withColumnRenamed('P14_POP', 'Population_2014')\n",
    "# on peut supprimer une colonne :\n",
    "data_col = data_col.drop(\"LIBGEO\", \"Population_2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+--------------------+\n",
      "| P14_POP|startswith(LIBGEO, Paris)|              LIBGEO|\n",
      "+--------+-------------------------+--------------------+\n",
      "| 21263.0|                     true|Paris 2e Arrondis...|\n",
      "| 26796.0|                     true|Paris 4e Arrondis...|\n",
      "|165745.0|                     true|Paris 16e Arrondi...|\n",
      "|170186.0|                     true|Paris 17e Arrondi...|\n",
      "| 60030.0|                     true|Paris 5e Arrondis...|\n",
      "| 55486.0|                     true|Paris 7e Arrondis...|\n",
      "|182318.0|                     true|Paris 13e Arrondi...|\n",
      "|141230.0|                     true|Paris 14e Arrondi...|\n",
      "|195468.0|                     true|Paris 20e Arrondi...|\n",
      "|199135.0|                     true|Paris 18e Arrondi...|\n",
      "|187156.0|                     true|Paris 19e Arrondi...|\n",
      "| 35077.0|                     true|Paris 3e Arrondis...|\n",
      "| 43134.0|                     true|Paris 6e Arrondis...|\n",
      "| 38257.0|                     true|Paris 8e Arrondis...|\n",
      "| 59389.0|                     true|Paris 9e Arrondis...|\n",
      "| 92228.0|                     true|Paris 10e Arrondi...|\n",
      "|151542.0|                     true|Paris 11e Arrondi...|\n",
      "| 16717.0|                     true|Paris 1er Arrondi...|\n",
      "|143922.0|                     true|Paris 12e Arrondi...|\n",
      "|235366.0|                     true|Paris 15e Arrondi...|\n",
      "+--------+-------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# on peut filtrer les observations\n",
    "data_reduced.filter(data_reduced['startswith(LIBGEO, Paris)'] == True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons sélectionné uniquement les observations commençant par « Paris »,\n",
    "on obtient donc les 20 arrondissements et leurs populations.\n",
    "\n",
    "On peut alors sauver ces données sous forme de fichiers parquet ou json :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_reduced.select(\"P14_POP\",\"LIBGEO\").write.save(\"resultat.parquet\")\n",
    "data_reduced.select(\"P14_POP\",\"LIBGEO\").write.save(\"resultat.json\",format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Afficher des statistiques descriptives\n",
    "Spark permet aussi de calculer des statistiques sur les données en utilisant, par\n",
    "exemple, une opération groupby :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|DEP|        avg(MED14)|\n",
      "+---+------------------+\n",
      "| 78|27908.276609517692|\n",
      "| 91|25505.856457531612|\n",
      "| 93|18004.142505975004|\n",
      "| 94|23223.062544887238|\n",
      "| 92|27815.275831569437|\n",
      "| 77|23544.776856209497|\n",
      "| 95| 24901.96792722259|\n",
      "| 75|29629.178876815004|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# on utilise un groupBy par département et\n",
    "# on affiche le salaire médian moyen\n",
    "salaire_med_moy = data_idf.groupBy(\"DEP\").agg({\"MED14\" :\"mean\"})\n",
    "salaire_med_moy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEP</th>\n",
       "      <th>avg(MED14)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78</td>\n",
       "      <td>27908.276610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>25505.856458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93</td>\n",
       "      <td>18004.142506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>23223.062545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92</td>\n",
       "      <td>27815.275832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEP    avg(MED14)\n",
       "0   78  27908.276610\n",
       "1   91  25505.856458\n",
       "2   93  18004.142506\n",
       "3   94  23223.062545\n",
       "4   92  27815.275832"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on peut transformer le résultat en format Pandas\n",
    "salaire_med_moy_pandas = salaire_med_moy.toPandas()\n",
    "# on aura les sorties de Pandas\n",
    "salaire_med_moy_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombreuses opérations proches de celles de Pandas sont disponibles avec\n",
    "Spark. \n",
    "\n",
    "\n",
    "#### Terminer votre session Spark\n",
    "Une fois que vous avez terminé de travailler sur votre session Spark, vous pouvez la fermer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4 Le machine learning avec Spark\n",
    "\n",
    "#### Préparation des données\n",
    "\n",
    "Nous supposons que nous avons déjà créé notre session Spark. Nous devons\n",
    "maintenant récupérer nos données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/07 22:02:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# on crée une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "         .appName(\"Exemples avec Python et Spark SQL\") \\\n",
    "         .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on récupère les données telecom\n",
    "churn=spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(\"../data/telecom.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La phase de préparation qui suit est importante. Il s’agit de définir les variables explicatives (x) et la variable cible (y) tout en transformant les variables non adaptées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on importe une classe qui transforme les colonnes qualitatives en colonnes\n",
    "# sous forme d’entiers (équivalent de LabelEncoder de Scikit-Learn)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# on va transformer la colonne Churn? et on va la nommer Churn2\n",
    "indexer = StringIndexer(inputCol='Churn?', outputCol='Churn2').fit(churn)\n",
    "\n",
    "# on construit ensuite un vecteur rassemblant toutes les colonnes explicatives\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# on rassemble la liste des colonnes numériques que l’on va utiliser\n",
    "numericCols = ['Day Mins','Day Calls','Day Charge','Eve Mins',\n",
    "               'Eve Calls','Eve Charge','Night Mins','Night Calls',\n",
    "               'Night Charge','Intl Mins','Intl Calls']\n",
    "\n",
    "# on crée un objet qui rassemble toutes ces colonnes dans une colonne\n",
    "# nommée var_expl\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"var_expl\")\n",
    "\n",
    "# on divise le DataFrame initial (churn) en deux DataFrame représentant\n",
    "# respectivement 70% et 30% des données\n",
    "\n",
    "(trainingData, testData) = churn.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À la différence de Scikit-Learn, on va devoir nommer les groupes de variables en\n",
    "entrée et en sortie lors de la création de l’objet à partir de la classe du modèle. \n",
    "\n",
    "Les données doivent donc avoir le format spécifié dans l’objet. Par ailleurs, on utilise un format spécifique pour les variables explicatives qui sont toutes stockées dans une structure à l’intérieur du DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du modèle et du pipeline\n",
    "Nous pouvons créer notre modèle de forêt aléatoire ainsi que le pipeline associé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# on crée notre modèle\n",
    "model=RandomForestClassifier(labelCol=\"Churn2\", featuresCol=\"var_expl\",\n",
    "                             numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# on construit le pipeline qui est composé des 3 étapes dévelopées auparavant\n",
    "pipeline = Pipeline(stages=[indexer, assembler, model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajustement et validation du modèle\n",
    "Nous faisons les calculs sur les données d’apprentissage et testons sur les\n",
    "données de validation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajustement du modèle\n",
    "model = pipeline.fit(trainingData)\n",
    "# prévision sur les données de validation\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, Spark va créer de nouvelles colonnes dans nos données avec les\n",
    "prédictions (colonne prediction) et les probabilités de prédiction (colonne\n",
    "rawPrediction).\n",
    "\n",
    "Nous pouvons calculer des métriques comme l’AUC ou le pourcentage de bien\n",
    "classés (accuracy) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7575137686860707"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# cette classe calcule l’AUC de notre modèle\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",\n",
    "                                          labelCol=\"Churn2\")\n",
    "\n",
    "# on applique les données prédites à notre objet d’évalaution\n",
    "evaluator.evaluate(predictions)\n",
    "\n",
    "# L’AUC est affichée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8730314960629921"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on calcule l’accuracy manuellement\n",
    "accuracy = predictions.filter(predictions.Churn2==predictions.prediction)\\\n",
    "                        .count() / float(testData.count())\n",
    "accuracy\n",
    "# on obtient l’accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les métriques utilisées nous permettent de voir que notre modèle ressemble à\n",
    "celui de Scikit-Learn en termes de performance (il est moins bon car nous avons\n",
    "moins de variables explicatives).\n",
    "\n",
    "Nous avons effectué tous les calculs dans notre environnement big data. Le seul\n",
    "moment où les données sont revenues vers nous est situé à la fin, pour récupérer le\n",
    "résultat.\n",
    "\n",
    "Cet exemple illustre bien la simplicité de Spark. L’utilisation de Spark pour des\n",
    "tâches plus complexes demande plus de travail mais PySpark et les DataFrames\n",
    "rendent ce passage très aisé pour un data scientist à l’aise avec les outils de traitement\n",
    "de données de Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
